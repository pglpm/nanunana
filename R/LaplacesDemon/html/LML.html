<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>R: Logarithm of the Marginal Likelihood</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="R.css">
</head><body>

<table width="100%" summary="page for LML {LaplacesDemon}"><tr><td>LML {LaplacesDemon}</td><td align="right">R Documentation</td></tr></table>

<h2>Logarithm of the Marginal Likelihood</h2>

<h3>Description</h3>


<p>This function approximates the logarithm of the marginal likelihood
(LML), where the marginal likelihood is also called the integrated
likelihood or the prior predictive distribution of <i>y</i>
in Bayesian inference. The marginal likelihood is
</p>
<p align="center"><i>p(y)
    = integral p(y | Theta)p(Theta) d Theta</i></p>

<p>The prior predictive distribution indicates what <i>y</i>
should look like, given the model, before <i>y</i> has been
observed. The presence of the marginal likelihood of
<i>y</i> normalizes the joint posterior distribution,
<i>p(Theta|y)</i>, ensuring it is a proper
distribution and integrates to one (see <code><a href="../../LaplacesDemon/html/is.proper.html">is.proper</a></code>). The
marginal likelihood is the denominator of Bayes' theorem, and is often
omitted, serving as a constant of proportionality. Several methods of
approximation are available.
</p>


<h3>Usage</h3>

<pre>
LML(Model=NULL, Data=NULL, Modes=NULL, theta=NULL, LL=NULL, method="NSIS")
</pre>


<h3>Arguments</h3>


<table summary="R argblock">
<tr valign="top"><td><code>Model</code></td>
<td>
<p>This is the model specification for the model that was
updated either in <code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code>,
<code><a href="../../LaplacesDemon/html/LaplacesDemon.html">LaplacesDemon</a></code>, or
<code><a href="../../LaplacesDemon/html/LaplacesDemon.hpc.html">LaplacesDemon.hpc</a></code>. This argument does not need to be
specified for the <code>HME</code> or <code>NSIS</code> methods.</p>
</td></tr>
<tr valign="top"><td><code>Data</code></td>
<td>
<p>This is the list of data passed to the model
specification. This argument does not need to be specified for the
<code>HME</code> or <code>NSIS</code> methods.</p>
</td></tr>
<tr valign="top"><td><code>Modes</code></td>
<td>
<p>This is a vector of the posterior modes (or medians, in
the case of MCMC), and does not need to be specified for the
<code>HME</code> or <code>NSIS</code> methods.</p>
</td></tr>
<tr valign="top"><td><code>theta</code></td>
<td>
<p>This is a matrix of posterior samples (parameters only),
and is specified only with the <code>HME</code> or <code>NSIS</code> methods.</p>
</td></tr>
<tr valign="top"><td><code>LL</code></td>
<td>
<p>This is a vector of MCMC samples of the log-likelihood, and
is specified only with the <code>HME</code> or <code>NSIS</code> methods.</p>
</td></tr>
<tr valign="top"><td><code>method</code></td>
<td>
<p>The method may be &quot;HME&quot;, &quot;LME1&quot;, &quot;LME2&quot;, or &quot;NSIS&quot;, and
defaults to &quot;NSIS&quot;. &quot;HME&quot; approximates the logarithm of the marginal
likelihood with the harmonic mean. &quot;LME1&quot; approximates the Hessian
matrix with finite differences and uses the Laplace-Metropolis
Estimator. &quot;LME2&quot; approximates the Hessian matrix with a Koschal
design and uses the Laplace-Metropolis Estimator. &quot;NSIS&quot; estimates
the logarithm of the marginal likelihood with nonparametric
self-normalized importance sampling (NSIS).</p>
</td></tr>
</table>


<h3>Details</h3>


<p>Generally, a user of <code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code>,
<code><a href="../../LaplacesDemon/html/LaplacesDemon.html">LaplacesDemon</a></code>, or <code><a href="../../LaplacesDemon/html/LaplacesDemon.hpc.html">LaplacesDemon.hpc</a></code> does
not need to use the <code>LML</code> function, because these methods already
include it. However, <code>LML</code> may be called by the user, should the
user desire to estimate the logarithm of the marginal likelihood with
a different method, or with non-stationary chains. The
<code><a href="../../LaplacesDemon/html/LaplacesDemon.html">LaplacesDemon</a></code> and <code><a href="../../LaplacesDemon/html/LaplacesDemon.hpc.html">LaplacesDemon.hpc</a></code>
functions only call <code>LML</code> when all parameters are stationary, and
only with non-adaptive algorithms (DRM, MWG, RWM, t-walk, or SMG).
</p>
<p>The <code>HME</code> method, where HME stands for harmonic mean estimator,
of Newton-Raftery (1994) is the easiest, and therefore fastest,
estimation of the logarithm of the marginal likelihood. However, it is
an unreliable estimator and should be avoided, because small
likelihood values can overly influence the estimator. It is included
here for completeness. There is not a function in this package that
uses this method by default. Given <i>N</i> samples, the estimator is
<i>1 / [1/N sum_N exp(-LL)]</i>.
</p>
<p>The <code>LME1</code> method uses the Laplace-Metropolis Estimator (LME), in
which the estimation of the Hessian matrix is approximated numerically
using finite differences, as shown in Gelman et al. (2004, p. 313&ndash;314),
though with a tolerance of <i>1e-06</i>. It is the slowest method here,
though it returns an estimate in more cases than the other methods.
The supplied <code>Model</code> specification must be executed a number of
times equal to <i>k^2 x 2</i>, where <i>k</i> is the
number of parameters. In large dimensions, this is very slow. The
Laplace-Metropolis Estimator is inappropriate with hierarchical
models.
</p>
<p>The <code>LME2</code> method uses a version of the Laplace-Metropolis
Estimator (LME) in which the estimation of the Hessian matrix is
essentially the <code>fdHess</code> function in the <code>nlme</code> package,
converted for Laplace's Demon. The <code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code>
function uses <code>LME2</code> when it has converged and <code>sir=FALSE</code>,
in which case it uses the posterior modes, and is itself Laplace
Approximation. In large dimensions, <code>LME2</code> is very slow. The
Laplace-Metropolis Estimator is inappropriate with hierarchical models.
</p>
<p>The <code>NSIS</code> method is essentially the <code>MarginalLikelihood</code>
function in the <code>MargLikArrogance</code> package. After <code>HME</code>,
this is the fastest method available here. The
<code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code> function uses <code>NSIS</code> when it
has converged and <code>sir=TRUE</code>. The <code><a href="../../LaplacesDemon/html/LaplacesDemon.html">LaplacesDemon</a></code> and
<code><a href="../../LaplacesDemon/html/LaplacesDemon.hpc.html">LaplacesDemon.hpc</a></code> functions use <code>NSIS</code>.
</p>
<p>The Laplace-Metropolis Estimator (LME) is the logarithmic form of
equation 4 in Lewis and Raftery (1997). In a non-hierarchical model,
the marginal likelihood may easily be approximated with the
Laplace-Metropolis Estimator for model <i>m</i> as
</p>
<p align="center"><i>p(y|m)
  = (2*pi)^(d_m/2) |Sigma_m|^(1/2) p(y|Theta_m, m)p(Theta_m|m)</i></p>

<p>where <i>d</i> is the number of parameters and <i>Sigma</i> is
the inverse of the negative of the approximated Hessian matrix of
second derivatives.
</p>
<p>As a rough estimate of Kass and Raftery (1995), LME is worrisome when
the sample size of the data is less than five times the number of
parameters, and LME should be adequate in most problems when the
sample size of the data exceeds twenty times the number of parameters
(p. 778).
</p>


<h3>Value</h3>


<p><code>LML</code> returns a list with two components:
</p>
<table summary="R valueblock">
<tr valign="top"><td><code>LML</code></td>
<td>

<p>This is an approximation of the logarithm of the marginal
likelihood (LML), which is notoriously difficult to estimate. For this
reason, several methods are provided. The marginal likelihood is
useful when comparing models, such as with Bayes factors in the
<code><a href="../../LaplacesDemon/html/BayesFactor.html">BayesFactor</a></code> function. When the method fails, <code>NA</code>
is returned, and it is most likely that the joint posterior is
improper (see <code><a href="../../LaplacesDemon/html/is.proper.html">is.proper</a></code>).</p>
</td></tr>
<tr valign="top"><td><code>VarCov</code></td>
<td>

<p>This is a variance-covariance matrix, and is the negative inverse of
the Hessian matrix, if estimated. The <code>HME</code> and <code>NSIS</code>
methods do not estimate <code>VarCov</code>, and return <code>NA</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Byron Hall <a href="mailto:statisticat@gmail.com">statisticat@gmail.com</a></p>


<h3>References</h3>


<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). &quot;Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.&quot;. Chapman and
Hall, London.
</p>
<p>Hall, B. (2012). &quot;Laplace's Demon&quot;, STATISTICAT, LLC.
URL=<a href="http://www.statisticat.com/laplacesdemon.html">http://www.statisticat.com/laplacesdemon.html</a>
</p>
<p>Kass, R.E. and Raftery, A.E. (1995). &quot;Bayes Factors&quot;. Journal of the
American Statistical Association, 90(430), p. 773&ndash;795.
</p>
<p>Lewis, S.M. and Raftery, A.E. (1997). &quot;Estimating Bayes Factors via
Posterior Simulation with the Laplace-Metropolis Estimator&quot;. Journal
of the American Statistical Association, 92, p. 648&ndash;655.
</p>
<p>Newton, M.A. and Raftery, A.E. (1994). &quot;Approximate Bayesian
Inference by the Weighted Likelihood Bootstrap&quot;. Journal of the
Royal Statistical Society, Series B 3, p. 3&ndash;48.
</p>


<h3>See Also</h3>


<p><code><a href="../../LaplacesDemon/html/BayesFactor.html">BayesFactor</a></code>,
<code><a href="../../LaplacesDemon/html/is.proper.html">is.proper</a></code>,
<code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code>,
<code><a href="../../LaplacesDemon/html/LaplacesDemon.html">LaplacesDemon</a></code>, and
<code><a href="../../LaplacesDemon/html/LaplacesDemon.hpc.html">LaplacesDemon.hpc</a></code>.
</p>


<h3>Examples</h3>

<pre>
### If a model object were created and called Fit, then:
#
### Applying HME to an object of class demonoid or pmc:
#LML(LL=Fit$Deviance*(-1/2), method="HME")
#
### Applying LME1 to an object of class demonoid:
#LML(Model, MyData, Modes=apply(Fit$Posterior1, 2, median), method="LME1")
#
### Applying LME2 to an object of class demonoid
#LML(Model, MyData, Modes=apply(Fit$Posterior1, 2, median), method="LME2")
#
### Applying NSIS to an object of class demonoid
#LML(theta=Fit$Posterior1, LL=Fit$Deviance*-(1/2), method="NSIS")
#
### Applying LME1 to an object of class laplace:
#LML(Model, MyData, Modes=Fit$Summary1[,1], method="LME1")
#
### Applying LME2 to an object of class laplace
#LML(Model, MyData, Modes=Fit$Summary1[,1], method="LME2")
</pre>

<hr><div align="center">[Package <em>LaplacesDemon</em> version 12.08.06 <a href="00Index.html">Index</a>]</div>
</body></html>