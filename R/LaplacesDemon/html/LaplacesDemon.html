<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>R: Laplace's Demon</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="R.css">
</head><body>

<table width="100%" summary="page for LaplacesDemon {LaplacesDemon}"><tr><td>LaplacesDemon {LaplacesDemon}</td><td align="right">R Documentation</td></tr></table>

<h2>Laplace's Demon</h2>

<h3>Description</h3>


<p>The <code>LaplacesDemon</code> function is the main function of Laplace's
Demon. Given data, a model specification, and initial values,
<code>LaplacesDemon</code> maximizes the logarithm of the unnormalized joint
posterior density with MCMC and provides samples of the marginal
posterior distributions, deviance, and other monitored variables.
</p>
<p>The <code>LaplacesDemon.hpc</code> function extends Laplace's Demon to
parallel chains for multicore or cluster high performance computing.
</p>


<h3>Usage</h3>

<pre>
LaplacesDemon(Model, Data, Initial.Values, Covar=NULL, Iterations=100000,
     Status=1000, Thinning=100, Algorithm="RWM", Specs=NULL, ...)
LaplacesDemon.hpc(Model, Data, Initial.Values, Covar=NULL,
     Iterations=100000, Status=1000, Thinning=100, Algorithm="RWM",
     Specs=NULL, Chains=2, CPUs=2, Packages=NULL, Dyn.libs=NULL)
</pre>


<h3>Arguments</h3>


<table summary="R argblock">
<tr valign="top"><td><code>Model</code></td>
<td>
<p>This required argument receives the model from a
user-defined function. The user-defined function is where the model
is specified. <code>LaplacesDemon</code> passes two arguments to the model
function, <code>parms</code> and <code>Data</code>, and receives five arguments
from the model function: <code>LP</code> (the logarithm of the
unnormalized joint posterior), <code>Dev</code> (the deviance),
<code>Monitor</code> (the monitored variables), <code>yhat</code> (the variables
for posterior predictive checks), and <code>parm</code>, the vector of
parameters, which may be constrained in the model function. More
information on the Model specification function may be found in the
&quot;LaplacesDemon Tutorial&quot; vignette, and the <code><a href="../../LaplacesDemon/html/is.model.html">is.model</a></code>
function. Many examples of model specification functions may be
found in the &quot;Examples&quot; vignette.</p>
</td></tr>
<tr valign="top"><td><code>Data</code></td>
<td>
<p>This required argument accepts a list of data. The list of
data must contain <code>mon.names</code> which contains monitored variable
names, and must contain <code>parm.names</code> which contains parameter
names. The <code><a href="../../LaplacesDemon/html/as.parm.names.html">as.parm.names</a></code> function may be helpful for
preparing the data, and the <code><a href="../../LaplacesDemon/html/is.data.html">is.data</a></code> function may be
helpful for checking data.</p>
</td></tr>
<tr valign="top"><td><code>Initial.Values</code></td>
<td>
<p>This argument requires a vector of initial
values equal in length to the number of parameters. Each initial
value will be the starting point for an adaptive chain or a
non-adaptive Markov chain of a parameter. If all initial values are
set to zero, then Laplace's Demon will attempt to optimize the
initial values with the <code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code>
function. After Laplace's Demon finishes updating, it may be desired
to continue updating from where it left off. To continue, this
argument should receive the last iteration of the previous
update. For example, if Fit is the output object, then
<code>Initial.Values=as.initial.values(Fit)</code>. Initial values may be
generated randomly with the <code><a href="../../LaplacesDemon/html/GIV.html">GIV</a></code> function. Although
dispersed initial values are ideal for multiple chains in
<code>LaplacesDemon.hpc</code>, all chains must begin with the same vector
of initial values.</p>
</td></tr>
<tr valign="top"><td><code>Covar</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, but may otherwise
accept a <i>K x K</i> proposal covariance matrix for the
first adaptation of the proposal covariances, where <i>K</i> is the
number of dimensions (or parameters). The proposal covariance matrix
will be re-estimated with each adaptation according to the entire
history of all chains. When the model is updated for the first time,
<code>Covar=NULL</code> should be used, unless there is a better estimate
at the variance of each target distribution as well as the
associated covariances. Once Laplace's Demon has finished updating,
it may be desired to continue updating where it left off, in which
case the proposal covariance matrix from the last run can be input
into the next run. The covariance matrix may also be input from the
<code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code> function, if used.</p>
</td></tr>
<tr valign="top"><td><code>Iterations</code></td>
<td>
<p>This required argument accepts integers larger than
10, and determines the number of iterations that Laplace's Demon
will update the parameters while searching for target
distributions. The required amount of computer memory will increase
with <code>Iterations</code>. If computer memory is exceeded, then all
will be lost. The <code><a href="../../LaplacesDemon/html/Combine.html">Combine</a></code> function can be used later
to combine multiple updates.</p>
</td></tr>
<tr valign="top"><td><code>Status</code></td>
<td>

<p>This argument accepts integers between 1 and the number of
iterations, and indicates how often the user would like the status
of the number of iterations and proposal type (for example,
multivariate or single-component, or mixture, or subset) printed to
the screen. For example, if a model is updated for 1,000 iterations
and <code>Status=200</code>, then a status message will be printed at the
following iterations: 200, 400, 600, and 800 in
<code>LaplacesDemon</code>. The <code>LaplacesDemon.hpc</code> function
does not print the status during parallel processing.</p>
</td></tr>
<tr valign="top"><td><code>Thinning</code></td>
<td>
<p>This argument accepts integers between 1 and the
number of iterations, and indicates that every nth iteration will be
retained, while the other iterations are discarded. If
<code>Thinning=5</code>, then every 5th iteration will be
retained. Thinning is performed to reduce autocorrelation and the
number of marginal posterior samples.</p>
</td></tr>
<tr valign="top"><td><code>Algorithm</code></td>
<td>
<p>This argument accepts the abbreviated name of the
MCMC algorithm, which must appear in quotes. A list of MCMC
algorithms appears below in the Details section, and the
abbreviated name is in parenthesis.</p>
</td></tr>
<tr valign="top"><td><code>Specs</code></td>
<td>
<p>This argument defaults to <code>NULL</code>, and accepts a list
of specifications for the MCMC algorithm declared in the
<code>Algorithm</code> argument. The specifications associated with each
algorithm may be seen below in the examples, and are described in
the details section below.</p>
</td></tr>
<tr valign="top"><td><code>Chains</code></td>
<td>
<p>This argument is required only for
<code>LaplacesDemon.hpc</code>, and indicates the number of parallel
chains.</p>
</td></tr>
<tr valign="top"><td><code>CPUs</code></td>
<td>
<p>This argument is required only for
<code>LaplacesDemon.hpc</code>, and indicates the number of central
processing units (CPUs) of the computer or cluster. For example,
when a user has a quad-core computer, <code>CPUs=4</code>.</p>
</td></tr>
<tr valign="top"><td><code>Packages</code></td>
<td>
<p>This optional argument is for use only with
<code>LaplacesDemon.hpc</code>, and defaults to <code>NULL</code>. This argument
accepts a vector of package names to load into each parallel chain.
If the <code>Model</code> specification depends on any packages, then
these package names need to be in this vector.</p>
</td></tr>
<tr valign="top"><td><code>Dyn.libs</code></td>
<td>
<p>This optional argument is for use only with
<code>LaplacesDemon.hpc</code>, and defaults to <code>NULL</code>. This argument
accepts a vector of the names of dynamic link libraries (shared
objects) to load into each parallel chain. The libraries must be
located in the working directory.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>Additional arguments are unused.</p>
</td></tr>
</table>


<h3>Details</h3>


<p><code>LaplacesDemon</code> offers numerous MCMC algorithms for numerical
approximation in Bayesian inference. The algorithms are
</p>

<ul>
<li><p> Adaptive Hamiltonian Monte Carlo (AHMC)
</p>
</li>
<li><p> Adaptive Metropolis (AM)
</p>
</li>
<li><p> Adaptive-Mixture Metropolis (AMM)
</p>
</li>
<li><p> Adaptive Metropolis-within-Gibbs (AMWG)
</p>
</li>
<li><p> Delayed Rejection Adaptive Metropolis (DRAM)
</p>
</li>
<li><p> Delayed Rejection Metropolis (DRM)
</p>
</li>
<li><p> Hamiltonian Monte Carlo (HMC)
</p>
</li>
<li><p> Hamiltonian Monte Carlo with Dual-Averaging (HMCDA)
</p>
</li>
<li><p> Metropolis-within-Gibbs (MWG)
</p>
</li>
<li><p> No-U-Turn Sampler (NUTS)
</p>
</li>
<li><p> Robust Adaptive Metropolis (RAM)
</p>
</li>
<li><p> Random-Walk Metropolis (RWM)
</p>
</li>
<li><p> Sequential Adaptive Metropolis-within-Gibbs (SAMWG)
</p>
</li>
<li><p> Sequential Metropolis-within-Gibbs (SMWG)
</p>
</li>
<li><p> Tempered Hamiltonian Monte Carlo (THMC)
</p>
</li>
<li><p> t-walk (twalk)
</p>
</li>
<li><p> Updating Sequential Adaptive Metropolis-within-Gibbs (USAMWG)
</p>
</li>
<li><p> Updating Sequential Metropolis-within-Gibbs (USMWG)
</p>
</li></ul>

<p>It is a goal for the documentation in the <span class="pkg">LaplacesDemon</span> to be
extensive. However, details of MCMC algorithms are best explored in
the &quot;LaplacesDemon Tutorial&quot; vignette, and additional information on
MCMC may be found in the vignette entitled &quot;Bayesian
Inference&quot;. Algorithm specifications (<code>Specs</code>) are listed below:
</p>

<ul>
<li> <p><code>A</code> is the number of initial, adaptive iterations to be
discarded as burn-in, and is used in HMCDA and NUTS.
</p>
</li>
<li> <p><code>Adaptive</code> is the iteration in which adaptation begins,
and is used in AM, AMM, and DRAM. These algorithms adapt according
to an observed covariance matrix, and should sample before beginning
to adapt.
</p>
</li>
<li> <p><code>alpha.star</code> is the desired acceptance rate(s) in RAM, and
may be a scalar or a vector equal in length to the number of
targets. The recommended value is <code>alpha.star=0.234</code>.
</p>
</li>
<li> <p><code>at</code> affects the traverse move in twalk. <code>at=6</code> is
recommended. It helps when some parameters are highly correlated,
and the correlation structure may change through the
state-space. The traverse move is associated with an acceptance rate
that decreases as the number of parameters increases, and is the
reason that <code>n1</code> is used to select a subset of parameters each
iteration. If adjusted, it is recommended to stay in the interval
[2,10].
</p>
</li>
<li> <p><code>aw</code> affects the walk move in twalk, and <code>aw=1.5</code> is
recommended. If adjusted, it is recommended to stay in the
interval [0.3,2].
</p>
</li>
<li> <p><code>Begin</code> indicates the time-period in which to begin
updating (filtering or predicting) in the USAMWG and USMWG
algorithms.
</p>
</li>
<li> <p><code>delta</code> is the target acceptance rate in HMCDA and
NUTS. The recommended value is 0.65 in HMCDA and 0.6 in NUTS.
</p>
</li>
<li> <p><code>Dist</code> is the proposal distribution in RAM, and may
either be <code>Dist="t"</code> for t-distributed or <code>Dist="N"</code> for
normally-distributed.
</p>
</li>
<li> <p><code>Dyn</code> is a <i>T x K</i> matrix of dynamic
parameters, where <i>T</i> is the number of time-periods and <i>K</i>
is the number of dynamic parameters. <code>Dyn</code> is used by SAMWG,
SMWG, USAMWG, and USMWG. Non-dynamic parameters are updated first in
each sampler iteration, then dynamic parameters are updated in a
random order in each time-period, and sequentially by time-period.
</p>
</li>
<li> <p><code>epsilon</code> is the step-size in AHMC, HMC, HMCDA, NUTS, and
THMC. It is a vector equal in length to the number of parameters in
AHMC, HMC, and THMC, and is a scalar in HMCDA and NUTS. When
<code>epsilon=NULL</code> in HMCDA or NUTS (only), a reasonable initial
value is found.
</p>
</li>
<li> <p><code>Fit</code> is an object of class <code>demonoid</code> in the USAMWG
and USMWG algorithms. Posterior samples before the time-period
specified in the <code>Begin</code> argument are not updated, and are used
instead from <code>Fit</code>.
</p>
</li>
<li> <p><code>gamma</code> controls the decay of adaptation in RAM. It is in
the interval (0.5,1], and 0.66 is recommended.
</p>
</li>
<li> <p><code>L</code> is a scalar number of leapfrog steps in AHMC, HMC, and
THMC. When <code>L=1</code>, the algorithm reduces to Langevin Monte Carlo
(LMC).
</p>
</li>
<li> <p><code>lambda</code> is a scalar trajectory length in HMCDA.
</p>
</li>
<li> <p><code>Lmax</code> is a scalar maximum for <code>L</code> (see above) in
HMCDA.
</p>
</li>
<li> <p><code>n1</code> affects the size of the subset of each set of points
to adjust, and is used in twalk. It relates to the number of
parameters, and <code>n1=4</code> is recommended. If adjusted, it is
recommended to stay in the interval [2,20].
</p>
</li>
<li> <p><code>Periodicity</code> specifies how often in iterations the
adaptive algorithm should adapt, and is used by AHMC, AM, AMM, AMWG,
DRAM, RAM, SAMWG, and USAMWG. If <code>Periodicity=10</code>, then the
algorithm adapts every 10th iteration. A higher <code>Periodicity</code>
is associated with an algorithm that runs faster, because it does
not have to calculate adaptation as often, though the algorithm
adapts less often to the target distributions, so it is a
trade-off. It is recommended to use the lowest value that runs fast
enough to suit the user, or provide sufficient adaptation.
</p>
</li>
<li> <p><code>SIV</code> stands for secondary initial values and is used by
twalk. <code>SIV</code> must be the same length as <code>Initial.Values</code>,
and each element of these two vectors must be unique from each
other, both before and after being passed to the <code>Model</code>
function. <code>SIV</code> defaults to <code>NULL</code>, in which case values
are generated with <code><a href="../../LaplacesDemon/html/GIV.html">GIV</a></code>.
</p>
</li>
<li> <p><code>Temperature</code> is used in the THMC algorithm to heat up
the momentum in the first half of the leapfrog steps, and then cool
down the momentum in the last half. <code>Temperature</code> must be
positive. When greater than 1, THMC should explore more diffuse
distributions, and may be helpful with multimodal distributions.
</p>
</li>
<li> <p><code>w</code> is a mixture weight for the AMM algorithm. It is in
the interval (0,1], and is recommended to use <code>w=0.05</code>, as per
Roberts and Rosenthal (2009). the The two mixture components are
adaptive multivariate and static/symmetric univariate proposals. The
mixture is determined at each iteration with mixture weight
<code>w</code>. A higher value of <code>w</code> is associated with more
static/symmetric univariate proposals, and a lower <code>w</code> is
associated with more adaptive multivariate proposals. AMM will be
unable to include the multivariate mixture component until it has
accumulated some history, and models with more parameters will take
longer to be able to use adaptive multivariate proposals.
</p>
</li></ul>



<h3>Value</h3>


<p><code>LaplacesDemon</code> returns an object of class <code>demonoid</code>, and
<code>LaplacesDemon.hpc</code> returns an object that is a list of objects of
class <code>demonoid.hpc</code>, where the number of components in the list
is the number of parallel chains. Each object of class <code>demonoid</code> 
is a list with the following components:
</p>
<table summary="R valueblock">
<tr valign="top"><td><code>Acceptance.Rate</code></td>
<td>
<p>This is the acceptance rate of the MCMC
algorithm, indicating the percentage of iterations in which the
proposals were accepted. The optimal acceptance rate varies with the
number of parameters, and by algorithm. Algorithms that update
single components have an optimal acceptance rate of 0.44,
regardless of the number of parameters. Algorithms that update with
multivariate proposals tend to have an optimal acceptance rate that
ranges from 0.44 for one parameter (one IID Gaussian target
distribution) to 0.234 for an infinite number of parameters (IID
Gaussian target distributions), and 0.234 is approached quickly as
the number of parameters increases. The AHMC, HMC, and THMC
algorithms have an optimal acceptance rate of 0.67, except when the
algorithm specification <code>L=1</code>, where the optimal acceptance
rate is 0.574.</p>
</td></tr>
<tr valign="top"><td><code>Adaptive</code></td>
<td>
<p>This reports the value of the <code>Adaptive</code>
argument.</p>
</td></tr>
<tr valign="top"><td><code>Algorithm</code></td>
<td>
<p>This reports the specific algorithm used.</p>
</td></tr>
<tr valign="top"><td><code>Call</code></td>
<td>
<p>This is the matched call of <code>LaplacesDemon</code>.</p>
</td></tr>
<tr valign="top"><td><code>Covar</code></td>
<td>
<p>This stores the <i>K x K</i> proposal
covariance matrix of the most recent adaptation, where <i>K</i> is
the dimension or number of parameters or initial values. If the
model is updated in the future, then this matrix can be used to
start the next update where the last update left off. Only the
diagonal of this matrix is reported in the associated <code>print</code>
function.</p>
</td></tr>
<tr valign="top"><td><code>CovarDHis</code></td>
<td>
<p>This <i>N x K</i> matrix stores the
diagonal of the proposal covariance matrix of each adaptation in
each of <i>N</i> rows for <i>K</i> dimensions, where the dimension is
the number of parameters or length of the initial values vector. The
proposal covariance matrix should change less over time. An
exception is that the AHMC algorithm stores an algorithm
specification here, which is not the diagonal of the proposal
covariance matrix.</p>
</td></tr>
<tr valign="top"><td><code>Deviance</code></td>
<td>
<p>This is a vector of the deviance of the model, with a
length equal to the number of thinned samples that were retained.
Deviance is useful for considering model fit, and is equal to the
sum of the log-likelihood for all rows in the data set, which is
then multiplied by negative two.</p>
</td></tr>
<tr valign="top"><td><code>DIC1</code></td>
<td>
<p>This is a vector of three values: Dbar, pD, and DIC. Dbar
is the mean deviance, pD is a measure of model complexity indicating
the effective number of parameters, and DIC is the Deviance
Information Criterion, which is a model fit statistic that is the
sum of Dbar and pD. <code>DIC1</code> is calculated over all retained
samples. Note that pD is calculated as <code>var(Deviance)/2</code> as in
Gelman et al. (2004).</p>
</td></tr>
<tr valign="top"><td><code>DIC2</code></td>
<td>
<p>This is identical to <code>DIC1</code> above, except that it is
calculated over only the samples that were considered by the
<code>Geweke.Diagnostic</code> to be stationary for all parameters. If
stationarity (or a lack of trend) is not estimated for all
parameters, then <code>DIC2</code> is set to missing values.</p>
</td></tr>
<tr valign="top"><td><code>DR</code></td>
<td>

<p>This reports the value of the <code>DR</code> argument.</p>
</td></tr>
<tr valign="top"><td><code>Initial.Values</code></td>
<td>
<p>This is the vector of <code>Initial.Values</code>,
which may have been optimized with the
<code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code> function.</p>
</td></tr>
<tr valign="top"><td><code>Iterations</code></td>
<td>
<p>This reports the number of <code>Iterations</code> for
updating.</p>
</td></tr>
<tr valign="top"><td><code>LML</code></td>
<td>
<p>This is an approximation of the logarithm of the marginal
likelihood of the data (see the <code><a href="../../LaplacesDemon/html/LML.html">LML</a></code> function for more
information). <code>LML</code> is estimated only with stationary samples,
and only with a non-adaptive algorithm, including Delayed Rejection
Metropolis (DRM), Hamiltonian Monte Carlo (HMC),
Metropolis-within-Gibbs (MWG), Random-Walk Metropolis (RWM),
Tempered Hamiltonian Monte Carlo (THMC) or t-walk (twalk).
<code>LML</code> is estimated with nonparametric self-normalized
importance sampling (NSIS), given LL and the marginal posterior
samples of the parameters. <code>LML</code> is useful for comparing
multiple models with the <code><a href="../../LaplacesDemon/html/BayesFactor.html">BayesFactor</a></code> function.</p>
</td></tr>
<tr valign="top"><td><code>Minutes</code></td>
<td>
<p>This indicates the number of minutes that
<code>LaplacesDemon</code> was running, and this includes the initial
checks as well as time it took the
<code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code> function, assessing stationarity,
effective sample size (ESS), and creating summaries.</p>
</td></tr>
<tr valign="top"><td><code>Model</code></td>
<td>
<p>This contains the model specification <code>Model</code>.</p>
</td></tr>
<tr valign="top"><td><code>Monitor</code></td>
<td>
<p>This is a vector or matrix of one or more monitored
variables, which are variables that were specified in the
<code>Model</code> function to be observed as chains (or Markov chains,
if <code>Adaptive=0</code>), but that were not deviance or parameters.</p>
</td></tr>
<tr valign="top"><td><code>Parameters</code></td>
<td>
<p>This reports the number of parameters.</p>
</td></tr>
<tr valign="top"><td><code>Periodicity</code></td>
<td>
<p>This reports the value of the <code>Periodicity</code>
argument.</p>
</td></tr>
<tr valign="top"><td><code>Posterior1</code></td>
<td>
<p>This is a matrix of marginal posterior distributions
composed of thinned samples, with a number of rows equal to the
number of thinned samples and a number of columns equal to the
number of parameters. This matrix includes all thinned samples.</p>
</td></tr>
<tr valign="top"><td><code>Posterior2</code></td>
<td>
<p>This is a matrix equal to <code>Posterior1</code>, except
that rows are included only if stationarity (a lack of trend) is
indicated by the <code><a href="../../LaplacesDemon/html/Geweke.Diagnostic.html">Geweke.Diagnostic</a></code> for all
parameters. If stationarity did not occur, then this matrix is
missing.</p>
</td></tr>
<tr valign="top"><td><code>Rec.BurnIn.Thinned</code></td>
<td>
<p>This is the recommended burn-in for the
thinned samples, where the value indicates the first row that was
stationary across all parameters, and previous rows are discarded
as burn-in. Samples considered as burn-in are discarded because they
do not represent the target distribution and have not adequately
forgotten the initial value of the chain (or Markov chain, if
<code>Adaptive=0</code>).</p>
</td></tr>
<tr valign="top"><td><code>Rec.BurnIn.UnThinned</code></td>
<td>
<p>This is the recommended burn-in for all
samples, in case thinning will not be necessary.</p>
</td></tr>
<tr valign="top"><td><code>Rec.Thinning</code></td>
<td>
<p>This is the recommended value for the
<code>Thinning</code> argument according to the autocorrelation in the
thinned samples, and it is limited to the interval [1,1000].</p>
</td></tr>
<tr valign="top"><td><code>Status</code></td>
<td>
<p>This is the value in the <code>Status</code> argument.</p>
</td></tr>
<tr valign="top"><td><code>Summary1</code></td>
<td>
<p>This is a matrix that summarizes the marginal
posterior distributions of the parameters, deviance, and monitored
variables over all samples in <code>Posterior1</code>. The following
summary statistics are included: mean, standard deviation, MCSE
(Monte Carlo Standard Error), ESS is the effective sample size due
to autocorrelation, and finally the 2.5%, 50%, and 97.5%
quantiles are reported. MCSE is essentially a standard deviation
around the marginal posterior mean that is due to uncertainty
associated with using MCMC. The acceptable size of the MCSE
depends on the acceptable uncertainty associated around the
marginal posterior mean. Laplace's Demon prefers to continue
updating until each MCSE is less than 6.27% of each marginal
posterior standard deviation (see the <code><a href="../../LaplacesDemon/html/MCSE.html">MCSE</a></code> and
<code><a href="../../LaplacesDemon/html/Consort.html">Consort</a></code> functions). The default <code>IMPS</code> method
is used. Next, the desired precision of ESS depends on the user's
goal, and Laplace's Demon prefers to continue until each ESS is at
least 100, which should be enough to describe 95% boundaries of an
approximately Gaussian distribution (see the <code><a href="../../LaplacesDemon/html/ESS.html">ESS</a></code> for
more information).</p>
</td></tr>
<tr valign="top"><td><code>Summary2</code></td>
<td>
<p>This matrix is identical to the matrix in
<code>Summary1</code>, except that it is calculated only on the
stationary samples found in <code>Posterior2</code>. If universal
stationarity was not estimated, then this matrix is set to
missing values.</p>
</td></tr>
<tr valign="top"><td><code>Thinned.Samples</code></td>
<td>
<p>This is the number of thinned samples that
were retained.</p>
</td></tr>
<tr valign="top"><td><code>Thinning</code></td>
<td>
<p>This is the value of the <code>Thinning</code> argument.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Byron Hall <a href="mailto:statisticat@gmail.com">statisticat@gmail.com</a></p>


<h3>References</h3>


<p>Christen, J.A. and Fox, C. (2010). &quot;A General Purpose Sampling
Algorithm for Continuous Distributions (the t-walk)&quot;. Bayesian
Analysis, 5(2), p. 263&ndash;282.
</p>
<p>Duane, S., Kennedy, A.D., Pendleton, B.J., and Roweth, D. (1987).
&quot;Hybrid Monte Carlo&quot;. Physics Letters, B, 195, p. 216&ndash;222.
</p>
<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). &quot;Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.&quot;. Chapman and
Hall, London.
</p>
<p>Hall, B. (2012). &quot;Laplace's Demon&quot;, STATISTICAT, LLC.
URL=<a href="http://www.statisticat.com/laplacesdemon.html">http://www.statisticat.com/laplacesdemon.html</a>
</p>
<p>Haario, H., Laine, M., Mira, A., and Saksman, E. (2006). &quot;DRAM:
Efficient Adaptive MCMC&quot;. Statistical Computing, 16, p. 339-354.
</p>
<p>Haario, H., Saksman, E., and Tamminen, J. (2001). &quot;An Adaptive
Metropolis Algorithm&quot;. Bernoulli, 7, p. 223-242.
</p>
<p>Hoffman, M.D. and Gelman. A. (2012). &quot;The No-U-Turn Sampler: Adaptively
Setting Path Lengths in Hamiltonian Monte Carlo&quot;. Journal of Machine
Learning Research, p. 1&ndash;30.
</p>
<p>Kass, R.E. and Raftery, A.E. (1995). &quot;Bayes Factors&quot;. Journal of the
American Statistical Association, 90(430), p. 773&ndash;795.
</p>
<p>Lewis, S.M. and Raftery, A.E. (1997). &quot;Estimating Bayes Factors via
Posterior Simulation with the Laplace-Metropolis Estimator&quot;. Journal
of the American Statistical Association, 92, p. 648&ndash;655.
</p>
<p>Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., and Teller,
E. (1953). &quot;Equation of State Calculations by Fast Computing
Machines&quot;. Journal of Chemical Physics, 21, p. 1087-1092.
</p>
<p>Mira, A. (2001). &quot;On Metropolis-Hastings Algorithms with Delayed
Rejection&quot;. Metron, Vol. LIX, n. 3-4, p. 231-241.
</p>
<p>Roberts, G.O. and Rosenthal, J.S. (2009). &quot;Examples of Adaptive
MCMC&quot;. Computational Statistics and Data Analysis, 18, p. 349&ndash;367.
</p>
<p>Rosenthal, J.S. (2007). &quot;AMCMC: An R interface for adaptive MCMC&quot;.
Computational Statistics and Data Analysis, 51, p. 5467&ndash;5470.
</p>
<p>Vihola, M. (2011). &quot;Robust Adaptive Metropolis Algorithm with Coerced
Acceptance Rate&quot;. Statistics and Computing. Springer, Netherlands.
</p>


<h3>See Also</h3>


<p><code><a href="../../LaplacesDemon/html/as.initial.values.html">as.initial.values</a></code>,
<code><a href="../../LaplacesDemon/html/as.parm.names.html">as.parm.names</a></code>,
<code><a href="../../LaplacesDemon/html/BayesFactor.html">BayesFactor</a></code>,
<code><a href="../../LaplacesDemon/html/Combine.html">Combine</a></code>,
<code><a href="../../LaplacesDemon/html/Consort.html">Consort</a></code>,
<code><a href="../../LaplacesDemon/html/ESS.html">ESS</a></code>,
<code><a href="../../LaplacesDemon/html/Geweke.Diagnostic.html">Geweke.Diagnostic</a></code>,
<code><a href="../../LaplacesDemon/html/GIV.html">GIV</a></code>,
<code><a href="../../LaplacesDemon/html/is.data.html">is.data</a></code>,
<code><a href="../../LaplacesDemon/html/is.model.html">is.model</a></code>,
<code><a href="../../LaplacesDemon/html/LaplaceApproximation.html">LaplaceApproximation</a></code>,
<code><a href="../../LaplacesDemon/html/LaplacesDemon.RAM.html">LaplacesDemon.RAM</a></code>,
<code><a href="../../LaplacesDemon/html/LML.html">LML</a></code>, and
<code><a href="../../LaplacesDemon/html/MCSE.html">MCSE</a></code>.
</p>


<h3>Examples</h3>

<pre>
# The accompanying Examples vignette is a compendium of examples.
####################  Load the LaplacesDemon Library  #####################
library(LaplacesDemon)

##############################  Demon Data  ###############################
data(demonsnacks)
y &lt;- log(demonsnacks$Calories)
X &lt;- cbind(1, as.matrix(demonsnacks[,c(7,8,10)]))
J &lt;- ncol(X)
for (j in 2:J) {X[,j] &lt;- CenterScale(X[,j])}
mon.names &lt;- c("LP","sigma")
parm.names &lt;- as.parm.names(list(beta=rep(0,J), log.sigma=0))
PGF &lt;- function(Data) return(c(rnormv(Data$J,0,1000),
     log(rhalfcauchy(1,25))))
MyData &lt;- list(J=J, PGF=PGF, X=X, mon.names=mon.names,
     parm.names=parm.names, y=y)

##########################  Model Specification  ##########################
Model &lt;- function(parm, Data)
     {
     ### Parameters
     beta &lt;- parm[1:Data$J]
     sigma &lt;- exp(parm[Data$J+1])
     ### Log of Prior Densities
     beta.prior &lt;- sum(dnormv(beta, 0, 1000, log=TRUE))
     sigma.prior &lt;- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu &lt;- tcrossprod(Data$X, t(beta))
     LL &lt;- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP &lt;- LL + beta.prior + sigma.prior
     Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma),
          yhat=rnorm(length(mu), mu, sigma), parm=parm)
     return(Modelout)
     }

set.seed(666)

############################  Initial Values  #############################
Initial.Values &lt;- GIV(Model, MyData, PGF=TRUE)

###########################################################################
# Examples of MCMC Algorithms                                             #
###########################################################################

######################  Adaptive-Mixture Metropolis  ######################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="AMM", Specs=list(Adaptive=500, Periodicity=10, w=0.05))
Fit
print(Fit)
Consort(Fit)
PosteriorChecks(Fit)
caterpillar.plot(Fit, Parms="beta")
BurnIn &lt;- Fit$Rec.BurnIn.Thinned
plot(Fit, BurnIn, MyData, PDF=FALSE)
Pred &lt;- predict(Fit, Model, MyData)
summary(Pred, Discrep="Chi-Square")
plot(Pred, Style="Covariates", Data=MyData)
plot(Pred, Style="Density", Rows=1:9)
plot(Pred, Style="ECDF")
plot(Pred, Style="Fitted")
plot(Pred, Style="Jarque-Bera")
plot(Pred, Style="Predictive Quantiles")
plot(Pred, Style="Residual Density")
plot(Pred, Style="Residuals")
Levene.Test(Pred)
Importance(Fit, Model, MyData, Discrep="Chi-Square")

##################  Adaptive Hamiltonian Monte Carlo  #####################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="AHMC", Specs=list(epsilon=rep(0.02, length(Initial.Values)),
     L=2, Periodicity=10))

##########################  Adaptive Metropolis  ##########################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="AM", Specs=list(Adaptive=500, Periodicity=10))

###################  Adaptive Metropolis-within-Gibbs  ####################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="AMWG", Specs=list(Periodicity=50))

#################  Delayed Rejection Adaptive Metropolis  #################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="DRAM", Specs=list(Adaptive=500, Periodicity=10))

#####################  Delayed Rejection Metropolis  ######################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="DRM", Specs=NULL)

#######################  Hamiltonian Monte Carlo  #########################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="HMC", Specs=list(epsilon=rep(0.02, length(Initial.Values)),
     L=2))

#############  Hamiltonian Monte Carlo with Dual-Averaging  ###############
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="HMCDA", Specs=list(A=500, delta=0.65, epsilon=NULL,
     Lmax=1000, lambda=0.1))

#######################  Metropolis-within-Gibbs  #########################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="MWG", Specs=NULL)

##########################  No-U-Turn Sampler  ############################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=100, Status=10, Thinning=1,
     Algorithm="NUTS", Specs=list(A=50, delta=0.6, epsilon=NULL))

######################  Robust Adaptive Metropolis  #######################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="RAM", Specs=list(alpha.star=0.234, Dist="N", gamma=0.66,
     Periodicity=10))

########################  Random-Walk Metropolis  #########################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="RWM", Specs=NULL)

##############  Sequential Adaptive Metropolis-within-Gibbs  ##############
#NOTE: The SAMWG algorithm is only for state-space models (SSMs)
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="SAMWG", Specs=list(Dyn=Dyn, Periodicity=50))

##################  Sequential Metropolis-within-Gibbs  ###################
#NOTE: The SMWG algorithm is only for state-space models (SSMs)
#Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
#     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
#     Algorithm="SMWG", Specs=list(Dyn=Dyn))

###################  Tempered Hamiltonian Monte Carlo  ####################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="THMC", Specs=list(epsilon=rep(0.05,length(Initial.Values)),
     L=2, Temperature=2))

###############################  t-walk  #################################
Fit &lt;- LaplacesDemon(Model, Data=MyData, Initial.Values,
     Covar=NULL, Iterations=1000, Status=100, Thinning=1,
     Algorithm="twalk", Specs=list(SIV=NULL, n1=4, at=6, aw=1.5))

##########  Updating Sequential Adaptive Metropolis-within-Gibbs  #########
#NOTE: The USAMWG algorithm is only for state-space model updating
#Fit &lt;- LaplacesDemon(Model, MyData, Initial.Values, 
#     Covar=NULL, Iterations=100000, Status=100, Thinning=100,
#     Algorithm="USAMWG", Specs=list(Dyn=Dyn, Periodicity=50, Fit=Fit,
#     Begin=T.m))

##############  Updating Sequential Metropolis-within-Gibbs  ##############
#NOTE: The USMWG algorithm is only for state-space model updating
#Fit &lt;- LaplacesDemon(Model, MyData, Initial.Values, 
#     Covar=NULL, Iterations=100000, Status=100, Thinning=100,
#     Algorithm="USMWG", Specs=list(Dyn=Dyn, Fit=Fit, Begin=T.m))

#End
</pre>

<hr><div align="center">[Package <em>LaplacesDemon</em> version 12.08.06 <a href="00Index.html">Index</a>]</div>
</body></html>