<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>R: Monte Carlo Standard Error</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="R.css">
</head><body>

<table width="100%" summary="page for MCSE {LaplacesDemon}"><tr><td>MCSE {LaplacesDemon}</td><td align="right">R Documentation</td></tr></table>

<h2>Monte Carlo Standard Error</h2>

<h3>Description</h3>


<p>Monte Carlo Standard Error (MCSE) is an estimate of the inaccuracy of
Monte Carlo samples, usually regarding the expectation of posterior
samples, <i>E(theta)</i>, from Monte Carlo or
Markov chain Monte Carlo (MCMC) algorithms, such as with the
<code><a href="../../LaplacesDemon/html/LaplacesDemon.html">LaplacesDemon</a></code> or <code><a href="../../LaplacesDemon/html/LaplacesDemon.hpc.html">LaplacesDemon.hpc</a></code>
functions. MCSE approaches zero as the number of independent posterior
samples approaches infinity. MCSE is essentially a standard deviation
around the posterior mean of the samples,
<i>E(theta)</i>, due to uncertainty associated with
using an MCMC algorithm, or Monte Carlo methods in general.
</p>
<p>The acceptable size of the MCSE depends on the acceptable uncertainty
associated around the marginal posterior mean,
<i>E(theta)</i>, and the goal of inference. It has
been argued that MCSE is generally unimportant when the goal of
inference is <i>theta</i> rather than
<i>E(theta)</i> (Gelman et al., 2004, p. 277), and
that a sufficient <code><a href="../../LaplacesDemon/html/ESS.html">ESS</a></code> is more important. Others perceive
MCSE to be a vital part of reporting any Bayesian model, and as a
stopping rule (Flegal et al., 2008).
</p>
<p>In Laplace's Demon, MCSE is part of the posterior summaries because it
is easy to estimate, and Laplace's Demon prefers to continue updating
until each MCSE is less than 6.27% of its associated marginal
posterior standard deviation (for more information on this stopping
rule, see the <code><a href="../../LaplacesDemon/html/Consort.html">Consort</a></code> function), since MCSE has been
demonstrated to be an excellent stopping rule.
</p>
<p>Acceptable error may be specified, if known, in the <code>MCSS</code>
(Monte Carlo Sample Size) function to estimate the required number of
posterior samples.
</p>
<p><code>MCSE</code> is a univariate function that is often applied to each
marginal posterior distribution. A multivariate form is not
included. By chance alone due to multiple independent tests, 5% of
the parameters should indicate unacceptable MSCEs, even when
acceptable. Assessing convergence is difficult.
</p>


<h3>Usage</h3>

<pre>
MCSE(x, method="IMPS", batch.size="sqrt", warn=FALSE)
MCSS(x, a)
</pre>


<h3>Arguments</h3>


<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>This is a vector of posterior samples for which MCSE or MCSS
will be estimated.</p>
</td></tr>
<tr valign="top"><td><code>a</code></td>
<td>
<p>This is a scalar argument of acceptable error for the mean of
<code>x</code>, and <code>a</code> must be positive. As acceptable error
decreases, the required number of samples increases.</p>
</td></tr>
<tr valign="top"><td><code>method</code></td>
<td>
<p>This is an optional argument for the method of MCSE
estimation, and defaults to Geyer's <code>"IMPS"</code> method. Optional
methods include <code>"sample.variance"</code> and <code>"batch.mean"</code>.
Note that <code>"batch.mean"</code> is recommended only when the number of
posterior samples is at least 1,000.</p>
</td></tr>
<tr valign="top"><td><code>batch.size</code></td>
<td>
<p>This is an optional argument that corresponds only
with <code>method="batch.means"</code>, and determines either the size of
the batches (accepting a numerical argument) or the method of
creating the size of batches, which is either <code>"sqrt"</code> or
<code>"cuberoot"</code>, and refers to the length of <code>x</code>. The default
argument is <code>"sqrt"</code>.</p>
</td></tr>
<tr valign="top"><td><code>warn</code></td>
<td>
<p>Logical. If <code>warn=TRUE</code>, then a warning is provided
with <code>method="batch.means"</code> whenever posterior sample size is
less than 1,000, or a warning is produced when more autcovariance
is recommended with <code>method="IMPS"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>


<p>The default method for estimating MCSE is Geyer's Initial Monotone
Positive Sequence (IMPS) estimator (Geyer, 1992), which takes the
asymptotic variance into account and is time-series based. This method
goes by other names, such as Initial Positive Sequence (IPS).
</p>
<p>The simplest method for estimating MCSE is to modify the formula for
standard error, <i>sigma(x) /
  sqrt(N)</i>, to account for non-independence in the sequence
<i>x</i> of posterior samples. Non-independence is
estimated with the <code>ESS</code> function for Effective Sample Size (see
the <code><a href="../../LaplacesDemon/html/ESS.html">ESS</a></code> function for more details), where <i>m = ESS(x)</i>, and MCSE is
<i>sigma(x) / sqrt(M)</i>. Although this
is the fastest and easiest method of estimation, it does not
incorporate an estimate of the asymptotic variance of
<i>x</i>.
</p>
<p>The batch means method (Jones et al., 2006; Flegal et al., 2008)
separates elements of <i>x</i> into batches and estimates
MCSE as a function of multiple batches. This method is excellent, but
is not recommended when the number of posterior samples is less than
1,000. These journal articles also assert that MCSE is a better
stopping rule than MCMC convergence diagnostics.
</p>
<p>The <code>MCSS</code> function estimates the required number of posterior
samples, given the user-specified acceptable error, posterior samples
<code>x</code>, and the observed variance (rather than asymptotic
variance). Due to the observed variance, this is a rough estimate.
</p>


<h3>Author(s)</h3>

<p>Byron Hall <a href="mailto:statisticat@gmail.com">statisticat@gmail.com</a></p>


<h3>References</h3>


<p>Flegal, J.M., Haran, M., and Jones, G.L. (2008). &quot;Markov chain Monte
Carlo: Can We Trust the Third Significant Figure?&quot;. Statistical
Science, 23, p. 250&ndash;260.
</p>
<p>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2004). &quot;Bayesian
Data Analysis, Texts in Statistical Science, 2nd ed.&quot;. Chapman and
Hall, London.
</p>
<p>Geyer, C.J. (1992). &quot;Practical Markov Chain Monte Carlo&quot;. Statistical
Science, 7, 4, p. 473&ndash;483.
</p>
<p>Hall, B. (2012). &quot;Laplace's Demon&quot;, STATISTICAT, LLC.
URL=<a href="http://www.statisticat.com/laplacesdemon.html">http://www.statisticat.com/laplacesdemon.html</a>
</p>
<p>Jones, G.L., Haran, M., Caffo, B.S., and Neath, R. (2006). &quot;Fixed-Width
Output Analysis for Markov chain Monte Carlo&quot;. Journal of the American
Statistical Association, 101, 1, p. 1537&ndash;1547.
</p>


<h3>See Also</h3>


<p><code><a href="../../LaplacesDemon/html/Consort.html">Consort</a></code>,
<code><a href="../../LaplacesDemon/html/ESS.html">ESS</a></code>,
<code><a href="../../LaplacesDemon/html/LaplacesDemon.html">LaplacesDemon</a></code>, and
<code><a href="../../LaplacesDemon/html/LaplacesDemon.hpc.html">LaplacesDemon.hpc</a></code>.
</p>


<h3>Examples</h3>

<pre>
library(LaplacesDemon)
x &lt;- rnorm(1000)
MCSE(x)
MCSE(x, method="batch.means")
MCSE(x, method="sample.variance")
MCSS(x, a=0.01)
</pre>

<hr><div align="center">[Package <em>LaplacesDemon</em> version 12.08.06 <a href="00Index.html">Index</a>]</div>
</body></html>